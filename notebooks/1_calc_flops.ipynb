{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a78c916e-fdd0-499b-9360-fc43ab1e422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import m2_utilities.flops as flops\n",
    "from m2_utilities.qwen import load_qwen\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2d4f859-644e-4aaa-939a-af62c7be3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_qwen()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7820ddb-e716-485f-94dc-7461159cf2ef",
   "metadata": {},
   "source": [
    "### Single Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d775204c-85a6-4cb3-93dc-ec692e57e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPS: 5.6518e+11\n"
     ]
    }
   ],
   "source": [
    "N_TOKENS = 512\n",
    "\n",
    "n_flops = flops.compute_flops(N_TOKENS, backpropagate=False)\n",
    "print(f\"Total FLOPS: {n_flops:.4e}\")\n",
    "\n",
    "# Total FLOPS: 5.6518e+11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732afb5-2bc5-498c-82b1-f7acc7135eb8",
   "metadata": {},
   "source": [
    "### Adding Poisitional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bd0a7f3-59f7-44af-a214-c83924566a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embbeding Layer: 4.59e+05\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.embedding(N_TOKENS, D_MODEL)\n",
    "print(f\"Embbeding Layer: {n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204c83d-da4c-4c36-97a2-cf1c02a703c4",
   "metadata": {},
   "source": [
    "### All Self-Attention Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99235dc5-2c00-4900-8821-1dc459fdf6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Block: 1.77e+10\n",
      "24 Blocks: 4.25e+11\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.block(N_TOKENS, N_HEADS, D_MODEL, HIDDEN_SIZE)\n",
    "print(f\"Single Block: {n_flops:.2e}\")\n",
    "print(f\"{N_LAYERS} Blocks: {N_LAYERS * n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbe2b7-9fc1-47a7-994d-d44c1cffb7aa",
   "metadata": {},
   "source": [
    "### Breakdown of a Single Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "003cf7f0-2277-43fb-ac70-773daca1a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN: 1.34e+10\n",
      "MHSA: 4.27e+09\n",
      "RMSNorm: 5.97e+06\n",
      "Residual: 4.59e+05\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.ffn(N_TOKENS, D_MODEL, HIDDEN_SIZE)\n",
    "print(f\"FFN: {n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.multi_head_self_attention(N_TOKENS, N_HEADS, D_MODEL)\n",
    "print(f\"MHSA: {n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.rms_norm(N_TOKENS, D_MODEL)\n",
    "print(f\"RMSNorm: {n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.add_residual(N_TOKENS, D_MODEL)\n",
    "print(f\"Residual: {n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3fd5e-7b99-4277-8aa9-834018f310e4",
   "metadata": {},
   "source": [
    "### Post Self-Attention Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9304559-c92b-4d7f-b6a8-44889d77edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Linear Transform: 1.39e+11\n",
      "Final Softmax: 4.77e+11\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.final_linear(N_TOKENS, D_MODEL, VOCAB_SIZE)\n",
    "print(f\"Final Linear Transform: { n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.softmax(N_TOKENS, VOCAB_SIZE)\n",
    "print(f\"Final Softmax: {n_flops:.2e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
