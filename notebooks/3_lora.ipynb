{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17211679-acc1-4229-9eab-b16f9d09e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from m2_utilities.load_data import load_trajectories\n",
    "from m2_utilities.preprocessor import stringify\n",
    "from m2_utilities.qwen import load_qwen\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2d7eac-8278-47ac-b6ec-b7335ea0952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear: nn.Linear, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "        assert isinstance(original_linear, nn.Linear)\n",
    "        self.original_linear = original_linear\n",
    "        self.original_linear.weight.requires_grad = False\n",
    "        if self.original_linear.bias is not None:\n",
    "            self.original_linear.bias.requires_grad = False\n",
    "        in_dim = original_linear.in_features\n",
    "        out_dim = original_linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha if alpha else r\n",
    "\n",
    "        device = original_linear.weight.device\n",
    "        self.A = nn.Parameter(torch.empty(r, in_dim, device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(out_dim, r, device=device))\n",
    "\n",
    "        # Initialise A with He initialization\n",
    "        nn.init.kaiming_normal_(self.A, nonlinearity=\"linear\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.original_linear(x)\n",
    "        lora_out = (x @ self.A.T) @ self.B.T\n",
    "        return base_out + lora_out * (self.alpha / self.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4674a1a0-39c1-4c70-9300-6d48ac1a57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_qwen()\n",
    "lora_rank = 4\n",
    "\n",
    "# Actually apply LoRA to the model:\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank)\n",
    "    layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb2b89e-bba2-4432-a772-deb1693321c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): LoRALinear(\n",
      "            (original_linear): Linear(in_features=896, out_features=896, bias=True)\n",
      "          )\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): LoRALinear(\n",
      "            (original_linear): Linear(in_features=896, out_features=128, bias=True)\n",
      "          )\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5dd83-ada8-4d03-8431-1527c3cfa9a3",
   "metadata": {},
   "source": [
    "### Loading Trajectories and Stringifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65f1683-c6d8-4471-b336-3771dafac68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_trajectories = load_trajectories(\"data/lotka_volterra_data.h5\")\n",
    "\n",
    "# Scaling\n",
    "ALPHA = 1.5\n",
    "trajectories = unscaled_trajectories / ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b77855-009c-4c5a-9fbf-f54ae6c31ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECIMALS = 2\n",
    "\n",
    "texts = []\n",
    "for trajectory in trajectories:\n",
    "    text = stringify(trajectory, DECIMALS)\n",
    "    texts.append(text)\n",
    "\n",
    "n_train = 10\n",
    "train_texts = texts[:10]\n",
    "val_texts = texts[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce83e51-5825-43c6-9503-842befb42492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified tokenization with chunking\n",
    "def process_sequences(texts, tokenizer, max_length=512, stride=256):\n",
    "    all_input_ids = []\n",
    "    for text in texts:\n",
    "        # Apply Qwen's tokenization scheme to the text:\n",
    "        encoding = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        seq_ids = encoding.input_ids[0]\n",
    "\n",
    "        # Create sliding windows to further divide the data into chunks:\n",
    "        for i in range(0, len(seq_ids), stride):\n",
    "            chunk = seq_ids[i : i + max_length]\n",
    "            if len(chunk) < max_length:\n",
    "                chunk = torch.cat(\n",
    "                    [\n",
    "                        chunk,\n",
    "                        torch.full((max_length - len(chunk),), tokenizer.pad_token_id),\n",
    "                    ]\n",
    "                )\n",
    "            all_input_ids.append(chunk)\n",
    "    return torch.stack(all_input_ids)\n",
    "\n",
    "\n",
    "max_ctx_length = 512\n",
    "train_input_ids = process_sequences(\n",
    "    train_texts, tokenizer, max_ctx_length, stride=max_ctx_length // 2\n",
    ")\n",
    "val_input_ids = process_sequences(\n",
    "    val_texts, tokenizer, max_ctx_length, stride=max_ctx_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3d4080-48e7-4523-b2ae-0293bc49f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=learning_rate\n",
    ")\n",
    "train_dataset = TensorDataset(train_input_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d01a3-8268-46a8-8281-79f523b38a6d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9be7c-8f6a-4caf-a039-056bfddc6701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "steps = 0\n",
    "while steps < 10000:\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Steps {steps}\")\n",
    "    for (batch,) in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        if steps > 10000:\n",
    "            break\n",
    "\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
