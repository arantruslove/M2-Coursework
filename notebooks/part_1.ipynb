{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78c916e-fdd0-499b-9360-fc43ab1e422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import m2_utilities.flops as flops\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7820ddb-e716-485f-94dc-7461159cf2ef",
   "metadata": {},
   "source": [
    "### Single Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d775204c-85a6-4cb3-93dc-ec692e57e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPS: 5.65e+11\n"
     ]
    }
   ],
   "source": [
    "N_TOKENS = 512\n",
    "N_LAYERS = 24\n",
    "N_HEADS = 14\n",
    "VOCAB_SIZE = 151646\n",
    "D_MODEL = 896\n",
    "HIDDEN_SIZE = 4864\n",
    "\n",
    "n_flops = flops.compute_flops(N_TOKENS, backpropagate=False)\n",
    "\n",
    "print(f\"Total FLOPS: {n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732afb5-2bc5-498c-82b1-f7acc7135eb8",
   "metadata": {},
   "source": [
    "### Adding Poisitional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bd0a7f3-59f7-44af-a214-c83924566a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embbeding Layer: 4.59e+05\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.embedding(N_TOKENS, D_MODEL)\n",
    "print(f\"Embbeding Layer: {n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204c83d-da4c-4c36-97a2-cf1c02a703c4",
   "metadata": {},
   "source": [
    "### All Self-Attention Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99235dc5-2c00-4900-8821-1dc459fdf6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Block: 1.77e+10\n",
      "24 Blocks: 4.25e+11\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.block(N_TOKENS, N_HEADS, D_MODEL, HIDDEN_SIZE)\n",
    "print(f\"Single Block: {n_flops:.2e}\")\n",
    "print(f\"{N_LAYERS} Blocks: {N_LAYERS * n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbe2b7-9fc1-47a7-994d-d44c1cffb7aa",
   "metadata": {},
   "source": [
    "### Breakdown of a Single Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "003cf7f0-2277-43fb-ac70-773daca1a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHSA: 4.27e+09\n",
      "FFN: 1.34e+10\n",
      "RMSNorm: 5.97e+06\n",
      "Add Residual: 4.59e+05\n"
     ]
    }
   ],
   "source": [
    "n_flops = flops.ffn(N_TOKENS, D_MODEL, HIDDEN_SIZE)\n",
    "print(f\"FFN: {n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.multi_head_self_attention(N_TOKENS, N_HEADS, D_MODEL)\n",
    "print(f\"MHSA: {n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.rms_norm(N_TOKENS, D_MODEL)\n",
    "print(f\"RMSNorm: {n_flops:.2e}\")\n",
    "\n",
    "n_flops = flops.add_residual(N_TOKENS, D_MODEL)\n",
    "print(f\"Add Residual: {n_flops:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3fd5e-7b99-4277-8aa9-834018f310e4",
   "metadata": {},
   "source": [
    "### Post Self-Attention Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9304559-c92b-4d7f-b6a8-44889d77edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Linear Transform: 1.39e+11\n",
      "Final Softmax: 9.32e+08\n"
     ]
    }
   ],
   "source": [
    "n_flops = N_TOKENS * flops.final_linear(D_MODEL, VOCAB_SIZE)\n",
    "print(f\"Final Linear Transform: { n_flops:.2e}\")\n",
    "\n",
    "n_flops = N_TOKENS * flops.softmax(VOCAB_SIZE)\n",
    "print(f\"Final Softmax: {n_flops:.2e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
