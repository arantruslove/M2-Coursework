hyperparam_options = {
    # Untuned Qwen2.5
    0: {
        "lr": None,
        "lora_rank": None,
        "chunk_size": None,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 850,
        "eval_interval": None,
        "max_steps": None,
    },
    # Initial training run
    1: {
        "lr": 1e-5,
        "lora_rank": 4,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 850,
        "eval_interval": 200,
        "max_steps": 1085,
    },
    # Hyperparameter grid search (2-12)
    2: {
        "lr": 1e-5,
        "lora_rank": 2,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    3: {
        "lr": 1e-5,
        "lora_rank": 4,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    4: {
        "lr": 1e-5,
        "lora_rank": 8,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    5: {
        "lr": 5e-5,
        "lora_rank": 2,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    6: {
        "lr": 5e-5,
        "lora_rank": 4,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    7: {
        "lr": 5e-5,
        "lora_rank": 8,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    8: {
        "lr": 1e-4,
        "lora_rank": 2,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    9: {
        "lr": 1e-4,
        "lora_rank": 4,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    10: {
        "lr": 1e-4,
        "lora_rank": 8,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    11: {
        "lr": 1e-4,
        "lora_rank": 8,
        "chunk_size": 128,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    12: {
        "lr": 1e-4,
        "lora_rank": 8,
        "chunk_size": 768,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 170,
        "eval_interval": float("inf"),
        "max_steps": 880,
    },
    # Final training run
    13: {
        "lr": 1e-4,
        "lora_rank": 8,
        "chunk_size": 512,
        "decimals": 2,
        "batch_size": 4,
        "train_size": 850,
        "eval_interval": 200,
        "max_steps": 2499,
    },
}
